name: PM001 爬虫任务

# 定义何时触发此工作流程
on:
  # 允许手动触发
  workflow_dispatch:
  # 定时触发，每天UTC时间22点(相当于北京时间次日6点)
  schedule:
    - cron: "0 22 * * *"

# 全局环境变量
env:
  PYTHON_VERSION: "3.9"
  # 使用shell命令定义日期，而不是复杂的format函数
  RUN_DATE: ${{ github.event.inputs.date || '' }}

# 定义工作流的权限
permissions:
  # 只需要内容写入权限
  contents: write

# 定义工作流中的作业
jobs:
  # 爬虫作业
  scrape-website:
    name: 运行网站爬虫
    runs-on: ubuntu-latest

    steps:
      # 步骤1: 检出代码
      - name: 检出仓库代码
        uses: actions/checkout@v4
        with:
          # 完整克隆以便进行Git操作
          fetch-depth: 0

      # 步骤2: 设置日期环境变量
      - name: 设置日期环境变量
        run: |
          echo "RUN_DATE=$(date -u +"%Y-%m-%d")" >> $GITHUB_ENV

      # 步骤3: 设置Python环境
      - name: 设置Python环境
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          # 启用依赖缓存
          cache: "pip"

      # 步骤4: 安装依赖
      - name: 安装依赖
        run: |
          python -m pip install --upgrade pip
          if [ -f requirements.txt ]; then
            pip install -r requirements.txt
          else
            echo "安装必要的依赖..."
            pip install requests beautifulsoup4
          fi

      # 步骤5: 创建数据目录
      - name: 创建数据目录
        run: |
          mkdir -p data/daily
          echo "创建日期目录: data/daily/${{ env.RUN_DATE }}"
          mkdir -p data/daily/${{ env.RUN_DATE }}

      # 步骤6: 运行爬虫脚本
      - name: 运行爬虫脚本
        id: run-scraper
        continue-on-error: true # 允许失败后继续执行工作流
        run: |
          echo "开始运行爬虫..."
          python 测试.py

          # 检查生成的文件
          if [ -f "pm001_recent_posts.tsv" ]; then
            echo "爬虫成功完成，发现结果文件"
            echo "file_exists=true" >> $GITHUB_OUTPUT
            echo "file_size=$(stat -c%s pm001_recent_posts.tsv)" >> $GITHUB_OUTPUT
            
            # 复制到日期目录
            cp pm001_recent_posts.tsv data/daily/${{ env.RUN_DATE }}/
            echo "数据文件已复制到日期目录"
          else
            echo "警告：未找到结果文件"
            echo "file_exists=false" >> $GITHUB_OUTPUT
            echo "file_size=0" >> $GITHUB_OUTPUT
          fi

      # 步骤7: 创建运行状态文件
      - name: 创建爬虫状态文件
        run: |
          mkdir -p .github/status
          # 创建状态文件
          if [ "${{ steps.run-scraper.outcome }}" == "success" ] && [ "${{ steps.run-scraper.outputs.file_exists }}" == "true" ]; then
            cat > .github/status/crawler_status.json << EOF
            {
              "status": "success",
              "date": "${{ env.RUN_DATE }}",
              "timestamp": "$(date -u +"%Y-%m-%dT%H:%M:%SZ")",
              "file_path": "data/daily/${{ env.RUN_DATE }}/pm001_recent_posts.tsv",
              "file_size": "${{ steps.run-scraper.outputs.file_size }}",
              "message": "爬虫运行成功，已生成数据文件"
            }
EOF
          else
            cat > .github/status/crawler_status.json << EOF
            {
              "status": "failed",
              "date": "${{ env.RUN_DATE }}",
              "timestamp": "$(date -u +"%Y-%m-%dT%H:%M:%SZ")",
              "message": "爬虫运行失败或未生成文件"
            }
EOF
          fi
          echo "已创建爬虫状态文件"

      # 步骤8: 提交结果和状态到仓库
      - name: 提交爬虫结果和状态
        run: |
          echo "正在提交爬虫结果和状态..."
          # 设置git配置
          git config user.name "github-actions[bot]"
          git config user.email "41898282+github-actions[bot]@users.noreply.github.com"

          # 添加需要提交的文件
          git add data/daily/${{ env.RUN_DATE }}/ || echo "没有数据目录变更"
          git add pm001_recent_posts.tsv || echo "没有主数据文件"
          git add .github/status/crawler_status.json

          # 检查是否有变更需要提交
          if git diff --staged --quiet; then
            echo "没有变更需要提交"
          else
            # 创建提交
            git commit -m "自动更新：爬虫数据 ${{ env.RUN_DATE }}"
            # 推送到仓库
            git push
            echo "成功提交并推送爬虫结果和状态"
          fi

      # 步骤9: 触发分析工作流
      - name: 触发分析工作流
        if: steps.run-scraper.outputs.file_exists == 'true'
        uses: benc-uk/workflow-dispatch@v1
        with:
          workflow: analyzer.yml
          token: ${{ secrets.GITHUB_TOKEN }}
          inputs: '{"data_date": "${{ env.RUN_DATE }}", "data_file": "data/daily/${{ env.RUN_DATE }}/pm001_recent_posts.tsv"}'
